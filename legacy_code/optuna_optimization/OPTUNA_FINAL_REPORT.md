# RelatÃ³rio Final: ImplementaÃ§Ã£o de OtimizaÃ§Ã£o com Optuna

## ğŸ“‹ Resumo Executivo

ImplementaÃ§Ã£o completa e funcional de otimizaÃ§Ã£o de hiperparÃ¢metros usando Optuna para todos os 4 modelos de previsÃ£o de vendas (ARIMA, Theta, Exponential Smoothing, XGBoost).

## âœ… O Que Foi Implementado

### 1. Infraestrutura Completa
- âœ… 4 otimizadores personalizados (um para cada modelo)
- âœ… Classe base `BaseOptimizer` com funcionalidades reutilizÃ¡veis
- âœ… IntegraÃ§Ã£o com TPE Sampler e MedianPruner
- âœ… PersistÃªncia em SQLite para estudos
- âœ… Dashboard interativo com Optuna Dashboard

### 2. Features AvanÃ§adas
- âœ… Cross-validation temporal automÃ¡tica
- âœ… Walk-forward validation para modelos Darts
- âœ… CÃ¡lculo de importÃ¢ncia de hiperparÃ¢metros
- âœ… Scripts de comparaÃ§Ã£o baseline vs otimizado
- âœ… Ajuste automÃ¡tico de n_splits baseado em tamanho do dataset

### 3. ConfiguraÃ§Ãµes Otimizadas
- âœ… EspaÃ§os de busca focados em parÃ¢metros importantes
- âœ… Ranges reduzidos para prevenir overfitting
- âœ… ConfiguraÃ§Ãµes especÃ­ficas para datasets pequenos

## ğŸ§ª Testes Realizados

### Teste 1: Baseline (20 trials, sem CV)
**Resultado**: Piora de -13.94% no MAE
- Causa: Overfitting no validation set
- Dataset: 120 pontos mensais
- Split inconsistente entre otimizaÃ§Ã£o e teste final

### Teste 2: Melhorado (50 trials, com CV)
**Melhorias Implementadas**:
1. âœ… Split consistente entre otimizaÃ§Ã£o e modelo
2. âœ… EspaÃ§o de busca reduzido (9 â†’ 9 parÃ¢metros, ranges focados)
3. âœ… Cross-validation temporal automÃ¡tica
4. âœ… Mais trials (20 â†’ 50)

**Resultados Parciais Observados**:
- Baseline MAE: ~6,356,730
- Melhor MAE CV: ~8,213,272 (Trial 45)
- Ainda acima do baseline devido a limitaÃ§Ãµes do dataset

## ğŸ“Š AnÃ¡lise de ImportÃ¢ncia de HiperparÃ¢metros

### XGBoost - ParÃ¢metros Mais Importantes:
1. **min_child_weight**: 66.66% ğŸ”¥
2. **n_estimators**: 10.40%
3. **gamma**: 9.12%
4. **colsample_bytree**: 3.81%
5. **learning_rate**: 2.80%

**Insight**: `min_child_weight` Ã© dominante - controla overfitting em datasets pequenos!

## âš ï¸ LimitaÃ§Ãµes Identificadas

### 1. Dataset Muito Pequeno
- **Problema**: Apenas 120 pontos mensais
- **Impacto**: Dificulta convergÃªncia da otimizaÃ§Ã£o
- **RecomendaÃ§Ã£o**: Coletar mais dados histÃ³ricos (idealmente 200+ pontos)

### 2. Alta VariÃ¢ncia
- **Problema**: CV scores variam muito entre folds
- **Impacto**: DifÃ­cil encontrar parÃ¢metros que generalizem
- **RecomendaÃ§Ã£o**: Feature engineering pode ser mais efetivo

### 3. Trade-off Bias-Variance
- **Problema**: ParÃ¢metros que reduzem training error aumentam test error
- **Impacto**: Overfitting Ã© difÃ­cil de evitar
- **RecomendaÃ§Ã£o**: Usar modelos mais simples ou regularizaÃ§Ã£o mais forte

## ğŸ’¡ RecomendaÃ§Ãµes por CenÃ¡rio

### CenÃ¡rio A: Dataset Pequeno (<150 pontos) - ATUAL
**RecomendaÃ§Ã£o**: Usar parÃ¢metros conservadores prÃ©-definidos

```python
XGBOOST_CONFIG_CONSERVATIVE = {
    "n_estimators": 1000,
    "max_depth": 4,  # Reduzido
    "learning_rate": 0.03,  # Reduzido
    "min_child_weight": 10,  # Aumentado
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "reg_alpha": 0.5,  # Aumentado
    "reg_lambda": 0.1,
}
```

**Alternativa**: Focar em:
1. Feature engineering (criar melhores features)
2. Ensemble de modelos simples
3. Transfer learning de datasets similares

### CenÃ¡rio B: Dataset MÃ©dio (150-500 pontos)
**RecomendaÃ§Ã£o**: Usar Optuna com espaÃ§o de busca reduzido

```bash
# 100 trials com CV
uv run python src/optimization/optimization_runner.py \
  --model xgboost \
  --n-trials 100 \
  --cv
```

### CenÃ¡rio C: Dataset Grande (>500 pontos)
**RecomendaÃ§Ã£o**: Usar Optuna com espaÃ§o de busca completo

```bash
# 200+ trials com CV
uv run python src/optimization/optimization_runner.py \
  --model xgboost \
  --n-trials 200 \
  --cv \
  --timeout 7200
```

## ğŸ¯ Como Usar a ImplementaÃ§Ã£o

### 1. OtimizaÃ§Ã£o Individual com CV
```bash
uv run python src/optimization/optimization_runner.py \
  --model xgboost \
  --n-trials 100 \
  --cv
```

### 2. Otimizar Todos os Modelos
```bash
uv run python src/optimization/optimization_runner.py \
  --model all \
  --n-trials 100 \
  --cv \
  --timeout 7200
```

### 3. Visualizar no Dashboard
```bash
optuna-dashboard sqlite:///optuna_studies.db
# Acesse: http://localhost:8080
```

### 4. Usar Melhores ParÃ¢metros
```python
from optimization import XGBoostOptimizer

# Load best parameters
optimizer = XGBoostOptimizer(config, data_path)
best_params = optimizer.load_best_params(
    Path("data/processed_data/xgboost/xgboost_best_params.json")
)

# Update config
config["xgboost"].update(best_params)

# Use in model
model = XGBoostForecaster(config)
```

## ğŸ“ˆ EspaÃ§os de Busca Implementados

### XGBoost - Focado (Recomendado para datasets pequenos)
```python
{
    "min_child_weight": [5, 15],      # Controla overfitting
    "n_estimators": [500, 2000],      # NÃºmero de Ã¡rvores
    "max_depth": [3, 6],               # Profundidade reduzida
    "learning_rate": [0.01, 0.1],     # Taxa de aprendizado
    "gamma": [0.0, 2.0],               # Penalidade de complexidade
    "subsample": [0.7, 0.9],           # Amostragem de dados
    "colsample_bytree": [0.7, 0.9],   # Amostragem de features
    "reg_alpha": [0.1, 2.0],           # L1 regularization
    "reg_lambda": [0.01, 0.5],         # L2 regularization
}
```

### XGBoost - Minimal (Para datasets muito pequenos <100 pontos)
```python
{
    "min_child_weight": [8, 12],
    "n_estimators": [800, 1600],
    "max_depth": [3, 5],
    "learning_rate": [0.02, 0.08],
}
```

## ğŸ”¬ Metodologia de OtimizaÃ§Ã£o

### TPE Sampler (Tree-structured Parzen Estimator)
- Modelo probabilÃ­stico do espaÃ§o de busca
- Mais eficiente que grid search ou random search
- Explora regiÃµes promissoras primeiro

### MedianPruner
- Interrompe trials ruins precocemente
- Economiza tempo computacional
- Baseado em performance mediana dos trials anteriores

### Cross-Validation Temporal
- Respeita ordem temporal dos dados
- Evita data leakage
- Ajuste automÃ¡tico de n_splits:
  - <80 pontos: 2 folds
  - 80-150 pontos: 3 folds
  - >150 pontos: 5 folds

## ğŸ“Š Estrutura de Arquivos

```
src/optimization/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_optimizer.py                    # Classe base
â”œâ”€â”€ xgboost_optimizer.py                 # Otimizador XGBoost
â”œâ”€â”€ arima_optimizer.py                   # Otimizador ARIMA
â”œâ”€â”€ theta_optimizer.py                   # Otimizador Theta
â”œâ”€â”€ exponential_smoothing_optimizer.py   # Otimizador ExpSmoothing
â””â”€â”€ optimization_runner.py               # Script principal CLI

Scripts de teste:
â”œâ”€â”€ test_optimized_models.py            # Teste rÃ¡pido
â”œâ”€â”€ compare_optimization.py             # ComparaÃ§Ã£o baseline vs otimizado
â””â”€â”€ test_improved_optimization.py       # Teste com melhorias

Resultados:
â”œâ”€â”€ optuna_studies.db                   # Database SQLite
â”œâ”€â”€ OPTIMIZATION_RESULTS.md             # Resultados iniciais
â””â”€â”€ OPTUNA_FINAL_REPORT.md              # Este relatÃ³rio
```

## ğŸ“ LiÃ§Ãµes Aprendidas

### 1. Size Matters
Dataset pequeno â†’ Hiperparameter tuning tem retorno limitado
Melhor investir em:
- Coletar mais dados
- Feature engineering
- Domain knowledge

### 2. Validation Strategy
CV temporal Ã© essencial para sÃ©ries temporais
Split simples pode levar a overfitting

### 3. Search Space Design
EspaÃ§o de busca deve ser ajustado ao tamanho do dataset
Ranges amplos demais â†’ dificulta convergÃªncia

### 4. Parameter Importance
AnÃ¡lise de importÃ¢ncia revela insights valiosos
Focar nos top 3-4 parÃ¢metros mais importantes

### 5. Computational Cost
OtimizaÃ§Ã£o com CV Ã© cara computacionalmente
Trade-off entre robustez e tempo de execuÃ§Ã£o

## ğŸš€ PrÃ³ximos Passos Sugeridos

### Para Este Projeto (Curto Prazo)
1. âœ… Documentar limitaÃ§Ãµes no TCC
2. âœ… Usar parÃ¢metros conservadores para baseline
3. âœ… Focar anÃ¡lise em comparaÃ§Ã£o entre modelos
4. âš ï¸ Considerar ensemble ao invÃ©s de single best

### Para ExtensÃµes Futuras (Longo Prazo)
1. ğŸ“Š Coletar mais dados histÃ³ricos (target: 200+ meses)
2. ğŸ”§ Testar outros algoritmos (LightGBM, CatBoost)
3. ğŸ¯ Implementar AutoML (FLAML, AutoGluon)
4. ğŸ”„ Continuous optimization em produÃ§Ã£o
5. ğŸ“ˆ Multi-objective optimization (accuracy + speed)

## âœ… ConclusÃ£o

A infraestrutura de otimizaÃ§Ã£o com Optuna foi **implementada com sucesso** e estÃ¡ **totalmente funcional**.

### Pontos Positivos:
- âœ… CÃ³digo modular e reutilizÃ¡vel
- âœ… Suporte a mÃºltiplos modelos
- âœ… Features avanÃ§adas (CV, pruning, importance)
- âœ… Dashboard interativo
- âœ… DocumentaÃ§Ã£o completa

### LimitaÃ§Ãµes Reais:
- âš ï¸ Dataset pequeno (120 pontos) limita eficÃ¡cia
- âš ï¸ Alta variÃ¢ncia nos resultados
- âš ï¸ Trade-off entre bias e variance difÃ­cil de resolver

### RecomendaÃ§Ã£o Final:
Para o TCC, **recomenda-se**:
1. Usar parÃ¢metros padrÃ£o cuidadosamente escolhidos
2. Focar anÃ¡lise na comparaÃ§Ã£o metodolÃ³gica entre modelos
3. Documentar limitaÃ§Ãµes do hyperparameter tuning com dataset pequeno
4. Citar a infraestrutura Optuna como contribuiÃ§Ã£o tÃ©cnica do trabalho

**A implementaÃ§Ã£o Optuna agrega valor ao TCC como:**
- DemonstraÃ§Ã£o de conhecimento de tÃ©cnicas modernas
- Infraestrutura reutilizÃ¡vel para trabalhos futuros
- AnÃ¡lise crÃ­tica de limitaÃ§Ãµes prÃ¡ticas

---

## ğŸ“š ReferÃªncias

- Akiba, T., et al. (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. KDD.
- Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. JMLR.
- Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: principles and practice.

---

**Autor**: Sistema de OtimizaÃ§Ã£o Implementado para TCC
**Data**: 2025-09-29
**VersÃ£o**: 1.0 - Final